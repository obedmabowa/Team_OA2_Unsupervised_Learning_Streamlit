# -*- coding: utf-8 -*-
"""Prototypee_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mJNFooEAivStOkCWwLWZPf0MDuGeqmMd

# Anime Recommender Project

![image.png](attachment:image.png)

Table of contents:
* [1. Project Overview](#chapter1)
    - [1.1. Introduction](#section_1_1)
    - [1.2. Problem Statement](#section_1_2)
    - [1.3. Objective of the Project](#section_1_3)
    - [1.4. Data Source](#section_1_4)
    - [1.5. Importance of the Study](#section_1_5)
    - [1.6. Key Questions or Hypotheses](#section_1_6)
    - [1.7. Methodology Overview](#section_1_7)
* [2. Importing Packages](#chapter2)
* [3. Loading Data](#chapter3)
* [4. Data Cleaning](#chapter4)
* [5. Exploratory Data Analysis (EDA))](#chapter5)
    - [5.1. Category Distribution](#section_5_1)
    - [5.2. Text Length Analysis](#section_5_2)
    - [5.3. Text Length Distribution by Category](#section_5_3)
    - [5.4. WordCloud](#section_5_4)
* [6. Data Preprocessing](#chapter6)
    - [6.1 Preprocessing Tasks for Classification](#section_6_1)
* [7. Modelling](#chapter7)
* [8. Model evaluation metrics](#chapter8)
* [9. Model performance analysis](#chapter9)
* [10. Conclusion](#chapter10)
* [11. References](#chapter11)

## 1. Project Overview <a class="anchor" id="chapter1"></a>

#### 1.1. Introduction <a class="anchor" id="section_1_1"></a>
-

#### 1.2. Problem Statement: <a id="section_1_2"></a>
-

#### 1.3. Objective of the Project <a id="section_1_3"></a>
-

#### 1.4. Data Source <a id="section_1_4"></a>
- The dataset used in this project comprises of three cvs namely anime.csv, test.csv and train.csv which contain a diverse collection of anime movies, each tagged with a specific category.
- The dataset includes multiple fields for each movie:

#### 1.5. Importance of the Study <a id="section_1_5"></a>.
-


#### 1.6. Key Questions or Hypotheses <a id="section_1_6"></a>

#### 1.7. Methodology Overview <a id="section_1_7"></a>
The methodology for this project includes several key steps:
- **Data Cleaning:** This step involves filtering out non-essential content to remove irrelevant information, managing incomplete data entries by handling missing values, standardizing the text format by normalizing text (e.g., converting to lowercase), and eliminating non-alphanumeric characters along with common, insignificant words through the removal of special characters and stop words.
- **Data Preprocessing:** This step involves feature extraction which is performed to transform raw text data into meaningful numerical representations. Techniques used include Term Frequency-Inverse Document Frequency (TF-IDF) for evaluating word importance in documents, word embeddings (such as Word2Vec and GloVe) for mapping words in vector space, and tokenization, which splits text into individual tokens for further analysis.
- **Exploratory Data Analysis (EDA):** This step involves analyzing the dataset to uncover patterns, trends, and relationships within the data. This step includes visualizing the distribution of categories, word frequency analysis, and other statistical measures.
- **Model Building:** This step involves training various machine learning models, including traditional algorithms like Logistic Regression, Support Vector Machines (SVM), and Random Forest, as well as advanced models like Convolutional Neural Networks (CNN) and Long Short-Term Memory networks (LSTM).
- **Model Evaluation:** This step involves assessing the performance of the models using metrics such as accuracy, precision, recall, and F1-score. This step involves fine-tuning hyperparameters and selecting the best-performing model based on the evaluation metrics.
- **Tools and Libraries:** Utilizing Python and its libraries, including Pandas for data manipulation, Numpy for numerical operations, Scikit-learn for machine learning algorithms, and TensorFlow/Keras for deep learning models.

## 2. Importing Packages <a class="anchor" id="chapter2"></a>
+ For data loading, manipulation, and analysis: `Pandas`, `csv`, `string`, `re`, `nltk`, `wordcloud` and `Numpy`.
+ For data visualization: `Matplotlib` and `Seaborn`

<div class="alert alert-block alert-info">
These libraries and tools collectively provide a comprehensive set of capabilities for handling data (pandas, numpy), manipulating text (re, nltk), and performing advanced natural language processing tasks (nltk). They are widely used in data science, machine learning, and text analytics projects due to their efficiency and versatility.
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy.sparse import hstack
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from difflib import get_close_matches
import requests
from bs4 import BeautifulSoup
import re

# Load datasets (replace with your actual file paths)
anime_df = pd.read_csv('anime.csv')
train_df = pd.read_csv('train.csv')

# Basic preprocessing (assuming genres and types are preprocessed and consistent)
anime_df.dropna(inplace=True)
train_df.dropna(inplace=True)

# Vectorize Genres
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(anime_df['genre'])

# Encode Type
label_encoder = LabelEncoder()
encoded_type = label_encoder.fit_transform(anime_df['type']).reshape(-1, 1)

# Calculate average rating per anime
average_rating = train_df.groupby('anime_id')['rating'].mean()
anime_df = anime_df.merge(average_rating, on='anime_id', how='left', suffixes=('', '_avg'))
anime_df['rating_avg'].fillna(0, inplace=True)  # Fill NaN values for animes without ratings

# Calculate number of ratings per anime
num_ratings = train_df.groupby('anime_id').size()
anime_df = anime_df.merge(pd.DataFrame(num_ratings, columns=['num_ratings']), on='anime_id', how='left')
anime_df['num_ratings'].fillna(0, inplace=True)  # Fill NaN values

# Normalize Numeric Features
scaler = StandardScaler()
anime_df[['rating_avg', 'num_ratings']] = scaler.fit_transform(anime_df[['rating_avg', 'num_ratings']])

# Combine All Features
combined_features = hstack([
    tfidf_matrix,
    encoded_type,
    anime_df[['rating_avg', 'num_ratings']].values
])

# Apply PCA for dimensionality reduction
pca = PCA(n_components=20, svd_solver='randomized')
combined_features_reduced_pca = pca.fit_transform(combined_features.toarray())

# Train KMeans on PCA-reduced features
kmeans_pca = KMeans(max_iter=900, n_clusters=25, n_init=30, random_state=42)
kmeans_pca.fit(combined_features_reduced_pca)

# Function to get recommendations
def recommend_anime_by_name(anime_name, kmeans_model, features_reduced, anime_df, num_recommendations=10):
    # Get the closest match for the anime name
    close_matches = get_close_matches(anime_name, anime_df['name'], n=1, cutoff=0.1)
    if not close_matches:
        return pd.DataFrame()  # Return empty DataFrame if no match is found

    matched_name = close_matches[0]
    anime_index = anime_df[anime_df['name'] == matched_name].index[0]

    # Predict the cluster for the given anime
    cluster_label = kmeans_model.predict(features_reduced[anime_index].reshape(1, -1))[0]

    # Get all anime indices belonging to the same cluster
    cluster_indices = np.where(kmeans_model.labels_ == cluster_label)[0]

    # Compute the cosine similarity matrix for the cluster
    cluster_features = features_reduced[cluster_indices]
    cosine_sim = cosine_similarity(cluster_features)

    # Get similarity scores for all animes in the same cluster
    sim_scores = list(enumerate(cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the top most similar animes
    sim_scores = sim_scores[1:num_recommendations + 1]
    anime_indices = [i[0] for i in sim_scores]

    # Return the recommended animes
    return anime_df.iloc[anime_indices]

# Function to get diverse recommendations by specific genres
def get_recommendations_by_genre(anime_df, genre, num_recommendations=6):
    genre_recommendations = anime_df[anime_df['genre'].str.contains(genre, case=False, na=False)].sample(num_recommendations)
    return genre_recommendations

# Function to fetch anime image URL from MyAnimeList
def fetch_image_url(anime_name):
    query = re.sub(r'\s+', '+', anime_name)  # Replace spaces with '+'
    url = f"https://myanimelist.net/search/all?q={query}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    img_tag = soup.find('img', class_='lazyload')
    if img_tag:
        return img_tag['data-src']
    return None

# Function to display anime information with image and info button
def display_anime_info_with_button(anime_row):
    image_url = fetch_image_url(anime_row['name'])
    if image_url:
        st.image(image_url, width=150)
    with st.expander("Show Info"):
        st.write(f"**Title:** {anime_row['name']}")
        st.write(f"**Genre:** {anime_row['genre']}")
        st.write(f"**Type:** {anime_row['type']}")
        st.write(f"**Rating:** {anime_row['rating_avg']:.2f}")
        st.write(f"**Number of Ratings:** {anime_row['num_ratings']}")

# Custom CSS for styling
st.markdown(
    """
    <style>
    /* Sidebar */
    .css-1aumxhk {
        background-color: #f0f0f0;
    }
    /* Main content */
    .css-1d391kg {
        background-color: #f9f9f9;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Streamlit App
st.markdown('<h1 style="text-align: center; color: green;">P̧͕̒̊͘ḣ̖̻͛̓o̯̱̊͊͢ẹ̿͋̒̕ṇ̤͛̒̍ỉ͔͖̜͌x̛̘̠̹͋A̷͙ͭͫ̕ṇ̤͛̒̍ỉ͔͖̜͌ḿ̬̏ͤͅẹ̿͋̒̕</h1>', unsafe_allow_html=True)


# Sidebar Navigation
st.sidebar.title("Navigation")
page = st.sidebar.radio("Go to", ["Home", "Search"])

# Home Page - Recommended for You
if page == "Home":
    st.write('## Recommended for You')

    # Get recommendations for specific genres
    comedy_recommendations = get_recommendations_by_genre(anime_df, 'Comedy', num_recommendations=6)
    action_recommendations = get_recommendations_by_genre(anime_df, 'Action', num_recommendations=6)
    romance_recommendations = get_recommendations_by_genre(anime_df, 'Romance', num_recommendations=6)

    # Display recommendations in rows by genre
    st.write("### Comedy")
    for idx, row in comedy_recommendations.iterrows():
        display_anime_info_with_button(row)
        st.write("---")

    st.write("### Action")
    for idx, row in action_recommendations.iterrows():
        display_anime_info_with_button(row)
        st.write("---")

    st.write("### Romance")
    for idx, row in romance_recommendations.iterrows():
        display_anime_info_with_button(row)
        st.write("---")

# Search Page - Anime Search
elif page == "Search":
    st.write('## Search for an Anime')

    # User input: Search for an anime
    search_anime = st.text_input('Search for an anime you like:')

    # Show recommendations based on search
    if search_anime:
        recommendations = recommend_anime_by_name(search_anime, kmeans_pca, combined_features_reduced_pca, anime_df, num_recommendations=10)
        if not recommendations.empty:
            st.write(f'Top 10 Recommendations for "{search_anime}":')
            for _, row in recommendations.iterrows():
                display_anime_info_with_button(row)
                st.write("---")
        else:
            st.write('No anime found with that name. Please try another name.')
